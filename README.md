# Model Serving
While the terms model serving and inference are sometimes used interchangeably they are not equivalent. Inference is using a trained model to make predictions on new or unseen data. Model serving is the process of deploying a trained model to make inference possible. In this repo we will walk you through 3 different ways to serve a trained model.

### Prerequsites
* Your own GCP account.
* Install [Google Cloud CLI](https://cloud.google.com/sdk?hl=en).
* Install Docker Desktop.
### Model Serving Alternatives
* [Local Serving via Docker](/local/README.md)
* [Serving via Cloud Run](/cloud_run/README.md)
* [Serving via Vertex AI](/vertex_ai/README.md)
